{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70584556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8d4b8b",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "## Ivan Makaveev, 2MI0600203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b601bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "np.random.seed(seed)\n",
    "\n",
    "mode = 'ALL'\n",
    "activator = 0\n",
    "hidden_count = 1\n",
    "neurons_per_layer = 4\n",
    "learning_rate = 0.1\n",
    "loss = \"BCE\" # MSE or BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39523794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_der(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_der(x):\n",
    "    return 1 - tanh(x) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aae0b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_derivative(y, y_predicted):\n",
    "    return -(y_predicted - y) # Derivative of Mean Squared Error\n",
    "\n",
    "def bce_loss_derivative(y, y_predicted):\n",
    "    return (y/y_predicted - (1-y)/(1-y_predicted)) # Derivative of Binary Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dfe2a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, input_size, activator, activator_der, learning_rate):\n",
    "        self.input_size = input_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activator = activator\n",
    "        self.activator_der = activator_der\n",
    "        \n",
    "        self.weights = np.random.normal(0, np.sqrt(1 / input_size), size=input_size)\n",
    "        self.bias = -1\n",
    "        self.gradientW = 0\n",
    "        self.gradientB = 0\n",
    "        self.record = np.zeros(input_size)\n",
    "        self.result = 0\n",
    "        \n",
    "    def forward(self, record):\n",
    "        self.record = record\n",
    "        self.result = np.dot(self.weights, record) + self.bias\n",
    "        return self.activator(self.result)\n",
    "    \n",
    "    def backward(self, propagated_error):\n",
    "        gradient = self.activator_der(self.result) * propagated_error\n",
    "        self.gradientW = gradient * self.record\n",
    "        self.gradientB = gradient\n",
    "        return gradient\n",
    "    \n",
    "    def update(self):\n",
    "        self.weights += self.learning_rate * self.gradientW\n",
    "        self.bias += self.learning_rate * self.gradientB\n",
    "        \n",
    "    def predict(self, record):\n",
    "        return self.activator(np.dot(self.weights, record) + self.bias)\n",
    "\n",
    "class NeuronNetwork:\n",
    "    def __init__(self, input_size, hidden_layers, neurons_per_layer, output_size, activator, lr, loss = \"MSE\"):\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.neurons_per_layer = neurons_per_layer\n",
    "        self.output_size = output_size\n",
    "        self.input_size = input_size\n",
    "        self.activator = sigmoid if activator == 0 else tanh\n",
    "        self.activator_der = sigmoid_der if activator == 0 else tanh_der\n",
    "        self.lr = lr\n",
    "        self.loss_der = mse_loss_derivative if loss == \"MSE\" else bce_loss_derivative\n",
    "        \n",
    "        self.layers = []\n",
    "        isInputConnected = False\n",
    "        for _ in range(hidden_layers):\n",
    "            curr_layer = []\n",
    "            for _ in range(neurons_per_layer):\n",
    "                curr_layer.append(Neuron(neurons_per_layer if isInputConnected else input_size, self.activator, self.activator_der, self.lr))\n",
    "            isInputConnected = True\n",
    "            self.layers.append(curr_layer)\n",
    "        \n",
    "        curr_layer = []\n",
    "        for _ in range(output_size):\n",
    "            curr_layer.append(Neuron(neurons_per_layer if isInputConnected else input_size, self.activator, self.activator_der, self.lr))\n",
    "        self.layers.append(curr_layer)\n",
    "\n",
    "    def train(self, data, epochs):\n",
    "        for _ in range(epochs):\n",
    "            for x, y in data:\n",
    "                layer_activations = [x]\n",
    "                for layer in self.layers:\n",
    "                    prev_act = layer_activations[-1]\n",
    "                    curr_layer_act = np.array([neuron.forward(prev_act) for neuron in layer], dtype=np.float64)\n",
    "                    layer_activations.append(curr_layer_act)\n",
    "\n",
    "                y_predicted = layer_activations[-1]\n",
    "\n",
    "                propagated_error = self.loss_der(y, y_predicted)\n",
    "                for layer_idx in range(len(self.layers) - 1, -1, -1):\n",
    "                    layer = self.layers[layer_idx]\n",
    "                    prev_input_size = self.layers[layer_idx][0].input_size\n",
    "\n",
    "                    grads = []\n",
    "                    for i, neuron in enumerate(layer):\n",
    "                        grad = neuron.backward(propagated_error[i])\n",
    "                        grads.append(grad)\n",
    "\n",
    "                    if layer_idx > 0:\n",
    "                        propagated_prev = np.zeros(prev_input_size, dtype=np.float64)\n",
    "                        for i, neuron in enumerate(layer):\n",
    "                            propagated_prev += propagated_error[i] * neuron.weights\n",
    "                        propagated_error = propagated_prev\n",
    "                        \n",
    "                    for neuron in layer:\n",
    "                        neuron.update()\n",
    "                        \n",
    "    def predict(self, record):\n",
    "        layer_activations = [record]\n",
    "        for layer in self.layers:\n",
    "            prev_act = layer_activations[-1]\n",
    "            curr_layer_act = np.array([neuron.predict(prev_act) for neuron in layer], dtype=np.float64)\n",
    "            layer_activations.append(curr_layer_act)\n",
    "                \n",
    "        return layer_activations[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67163d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bool_func(mode, inputs):\n",
    "    if(mode == 'OR'):\n",
    "        return [(i, i[0] | i[1])  for i in inputs]\n",
    "    if(mode == 'AND'):\n",
    "        return [(i, i[0] & i[1])  for i in inputs]\n",
    "    if(mode == 'XOR'):\n",
    "        return [(i, i[0] ^ i[1])  for i in inputs]\n",
    "    raise ValueError(\"Unknown mode\")\n",
    "    \n",
    "inputs = np.array(\n",
    "    [\n",
    "        [0, 0],\n",
    "        [0, 1],\n",
    "        [1, 0],\n",
    "        [1, 1]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fac8ac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_neural_net(mode):\n",
    "    data = get_bool_func(mode, inputs)\n",
    "    nn = NeuronNetwork(inputs.shape[1], hidden_count, neurons_per_layer, 1, activator, learning_rate, loss)\n",
    "    nn.train(data, 5000)\n",
    "    print(f\"Mode: {mode}\")\n",
    "    for i in inputs:\n",
    "        print(f\"Input {i} -> {nn.predict(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aba4d143",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: AND\n",
      "Input [0 0] -> [0.00036846]\n",
      "Input [0 1] -> [0.00038172]\n",
      "Input [1 0] -> [0.0003818]\n",
      "Input [1 1] -> [0.99853552]\n",
      "Mode: OR\n",
      "Input [0 0] -> [0.00090942]\n",
      "Input [0 1] -> [0.99968009]\n",
      "Input [1 0] -> [0.99968009]\n",
      "Input [1 1] -> [0.9996861]\n",
      "Mode: XOR\n",
      "Input [0 0] -> [0.00022307]\n",
      "Input [0 1] -> [0.99582875]\n",
      "Input [1 0] -> [0.99791442]\n",
      "Input [1 1] -> [0.00810905]\n"
     ]
    }
   ],
   "source": [
    "if(mode == \"ALL\"):\n",
    "    for m in (\"AND\", \"OR\", \"XOR\"):\n",
    "        test_neural_net(m)\n",
    "else:\n",
    "    test_neural_net(mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
